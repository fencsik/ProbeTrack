Tracking Project Notes and Musings
by David Fencsik

$LastChangedDate$

----------------------------------------------------------------------

December 16, 2003:

Potential follow-up studies:

Present target cues during movement: i.e., Ss identify the targets
while they're already moving, which eliminates need to switch from
immobile to mobile targets, but still have to track subset of
identical targets for 5 s.

step up speed in ShiftTrack3 to see if changes effect of -1 = 0 > 1.

Look at function of tracking with reappearance positions in the range
[-1,1]. But we need to be careful to achieve some power here.

Start putting an intervening task into the gap, or before/after the
tracking task. 

Orthogonally vary distance travelled and trajectory changes during
blank interval. 

Start playing with confusion just before disappearance: e.g., multiple
objects cross over, post-gap distractor is closer to pre-gap target
than the post-gap target. 

----------------------------------------------------------------------

December 18, 2003:

Well, what the heck do we know now? Results of ShiftTrack3 are that -1
and 0 are approx. equivalent, ...

----------------------------------------------------------------------

January 5, 2004:

The current state of affairs is that we have two hypotheses. The
location-based matching hypothesis claims that participants match the
post-gap objects to the nearest pre-gap targets. This hypothesis
requires a memory system (visWM?) to store all the object locations
from the pre-gap display, and then a little location-based matching
following reappearance. This hypothesis predicts that observers will
switch from a target to a non-target during a gap if the non-target is
closer.

The trajectory-matching hypothesis claims that participants keep track
of each object's trajectory, and use the trajectory to predict where
the objects should be when they reappear, then match post-gap objects
based on their distance from the predicted locations. 

One problem with previous studies is that people may have access to
both mechanisms. Thus, when objects reappear at an offset of 1 (i.e.,
at the position where they should be had they moved at a constant
velocity before and during the gap), participants do better if they
rely on trajectory-based matching (unless the movement is so slight
that location-based matching is just as good), whereas reappearance
at position 0 (i.e., the pre-gap locations) is best handled by
location-based matching (only). Perhaps we could encourage one
strategy over the other by using predominantly one type of
reappearance position over the other, but save that for later.

If location-based matching is really how this thing gets done, then we
can intentionally set things up so a non-target distracter will be
closer to a pre-gap target location than the corresponding target
(which has moved away), and ask the subject to pick between the target
and the non-target distracter. According to the location-based
matching account, participants should be unable to track the target
and always switch to the distracter.

----------------------------------------------------------------------

May 6, 2004: 

I had three ideas just before VSS (* indicates not from VSS):

1. ShiftTrack3 should be replicated with no movement following the
   gap. In that experiment, the advantage of Position -1 over Position
   1 may have occurred because in Position -1, the disk moved over
   Position 0 300 ms after the gap. Eliminating post-gap motion will
   remove this confound.
2. StopTrack2 needs to be replicated with a variable tracking
   interval. In the original experiment, trial length was fixed at 4
   sec. Since the gap always began 300 ms prior to the final frame,
   subjects could have anticipated the gap.
3. ShiftTrack studies could be run with all save one item shifting
   positions, and then test the one. This way, subjects might use one
   set for reacquisition, and we can test how effective that is when
   stimuli are inconsistent with their set. One difficulty with this
   is that the odd-one-out might cue which is the target.
4. *Extend range of targets tested in ShiftTrack and StopTrack.
5. *Change recall method, so they only have to make one decision:
   this makes capacity estimates easier. 

----------------------------------------------------------------------

August 12, 2004:

So far, we have established that location-based matching is preferred
(shifttrack studies), but that people can use motion information from
one object to aid target reacquisition.

There are two things that would be nice to achieve with this
paradigm: One would be a double-dissociation between location and
motion information; so far, we have only the single dissociation from
eliminating motion information in the StopTrack studies. Second would
be to determine whether people are using motion information to
constrain their search, or whether they are actually predicting
reappearance location.

On a related topic, one hypothesis we proposed to explain why people
were better at synchronous disappearance compared to asynchronous
disappearance was that synchronous disappearance mimics task
postponement, so observers might just pause the task, then resume
it. Perhaps we should separately study asynchronous disappearance to
find out how people maintain tracking information during a task
without postponement mechanisms coming into play. One question we may
be interested in, for example, is whether reappearance at position 1
is easier to reacquire than reappearance at position 0 with
asynchronous disappearance. 

----------------------------------------------------------------------

August 13, 2004:

Future possibilities:

1. Map out reacquisition probability between positions -1 and 1, and
   investigate perpendicular shifts during gap. This will give us a
   sense of how a moving object is represented during the gap. As a
   prerequisite, should try to improve effect sizes to Keane &
   Pylyshyn levels. Our current explanation for the difference is
   that we manipulate reappearance position between block and Keane &
   Pylyshyn did so within block.

2. Study target reacquisition in asynchronous disappearance case. See
   final paragraph from August 12, 2004. 

Jeremy's primary goal is to find interesting and novel things. His
suggestsions were:

1. Testing "what's hiding behind this bush" in NoahTrack. Is this
   programmed up already?

2. Testing whether different kinds of motion (e.g., brownian
   vs. ballistic) aid in discriminating targets from distracters.

3. He thought that #2 in the previous list might be worth
   exploring. Run something like ShiftTrack3 or 4 with asynchronous
   disappearance.

The fastest for me to program up is probably #3 from the second
list. #1 may be already programmed up, and I'll need Todd to send me
the latest code at least. #2 involves playing around with the two
movement vectors again. #3 just involves allowing for different gap
onset times for each disk. 

We begin with a basic replication of ShiftTrack3 with reappearance
positions -1, 0, and 1. Trial duration can be fixed (but perhaps a
bit longer, say 6 s). Block position and maybe manipulate ntargets (2
and 4?).

----------------------------------------------------------------------

August 19, 2004:

Regarding #1 from the first list from August 13, 2004, Todd points
out that this might be a worthwhile study to run, just to have it
available. 

----------------------------------------------------------------------

August 23, 2004:

Jeremy designed a new kind of MVT task in which targets are cued one
at a time while they are already moving.  Objects can be cued as
targets, then removed from the target set later.  Generally, this
allows us to study target acquisition and tracking processes.

Some specific question we might want to ask: How many targets can be
acquired/dropped simultaneously?  Can people simultaneously add some
targets while dropping others?  Is there any advantage to recalling
features from a recently dropped object as compared to an object that
was never a target?  How long does such an advantage last?  Can there
be a sudden switch from one group of targets to a non-overlapping set
of targets?  Does dropping/adding of targets become more difficult
with increasing tracking load?

What we need here are some good theories.  

----------------------------------------------------------------------

September 24, 2004:

Regarding Jeremy's task mentioned above: Can people maintain any
sort of order information regarding the targets? Is there any kind of
recency or primacy effects? Can they do an N-back sorta thing where
the only have to track the most recent N objects and drop the (N+1)th
back each time a new one is added?

----------------------------------------------------------------------

September 28, 2004:

Need to meet with Todd and Skyler regarding future directions for the
MVT projects.  Things to discuss:

* Brian's suggestions
* Future Noah direction: what's hiding behind this bush?
* "Killer ShiftTrack study": all reappear at position X except one,
  which reappears at position Y (Y != X). 
* Other ways to study preparatory set: adjust proportion of
  move/non-move/rewind trials within a block. 

These are the ideas we came up with at the meeting:

* Studies we want from Brian's suggestions:
  * Vary gap duration and reappearance position (e.g., move,
    non-move) to test memory duration for various kinds of info.
    Maybe Brian can run this one?
  * ShiftTrack with 1 vs. 4 targets, just to see if there's a bigger,
    qualitative difference than there was with 2 vs. 5 targets.  Gave
    this to Skyler to program up and run.
  * Contextual Cuing & Tracking: Ogawa and Yagi (2002) had a VSS
    poster (?) showing that repeating the trajectories of targets and
    distractors aids tracking performance (a la Jiang & Chun's
    contextual cuing in visual search).  Are there other "features"
    that might aid tracking: e.g., is it just target paths, just
    distractor paths, or both that need to be repeated (Ogawa &
    Yagi's results speak to this)?  Is it the trajectory or the final
    display that matters?  What if we rotate all the trajectories by
    90 degrees?
* Todd's new studies, which are still in development, but nearing
  deployment:
  * AllTrack: track everything and test fidelity of location memory.
    This is intended to provide a converging estimate of location
    capacity for the NoahTrack results.
  * ProbeTrack: track subset of objects and make a speeded
    target/non-target discrimination response to a probed object.
    Probe is presented at a variable delay following a gap.  The idea
    is to figure out how quickly Os reacquire targets following a gap.
* Jeremy's Pick-Up and Drop-Off (PUDO) tracking experiments:
  currently being run with three major conditions--standard tracking with
  simultaneously cued targets, target set gets built up one at a time
  (possibly with two different tracking durations), and targets get
  added and deleted during tracking.
* ShiftTrack "Killer" study: track 4/8, all-but-one reappear at
  position 1 (odd-one-out reappears at 0) and manipulate three
  conditions: odd-one-out is target or distractor, probe is
  target or distractor, and probe is odd-one-out or not.  I need to
  code this on up.
* NoahTrack: 
  * Run a version with a cue of "what is behind this bush?"  Possibly
    also change identities of stimuli during tracking.  
  * Run a version that allows us to estimate how many objects' worth
    of locations are stored separately from how many identities are
    stored.  This manipulation involves asking whether there is a
    target in a cued location vs. asking whether a particular animal
    was at that location.

----------------------------------------------------------------------

October 21, 2004:

Todd told me a of a recent meeting with Yuhong Jiang, in which she
suggested a new dual-task MOT meets visual search study.  The premise
is that people are able to overlap both tasks because they use the
Horowitz All-purpose Task-postponement (HAT) memory store to postpone
the tracking task, perform the search task, then switch back to the
tracking task.  The only detriment to performance would come from
uncertainty in locating the targets (which continued to move during
the gap) using their last known positions from the HAT store.

This uncertainty should be constant for any particular gap length, and
shouldn't depend on the presence or difficulty (?) of an intervening
task.

More to come...

----------------------------------------------------------------------

November 1, 2004

Findings since VSS:

Turns out that the weird equivalence between reappearance positions -1
and 0 is artifactual: ShiftTrack4 showed that -1 and 1 are equivalent
and below 0 when the stimuli stop moving immediately after the gap.

The accuracy pattern seems to change under asynchronous disappearance
conditions (ShiftTrack5).

Planned study: top-down influences on reacquisition strategies

----------------------------------------------------------------------

February 10, 2005:

One idea to further the StopTrack studies: the reason we see motion
information for only one object may be that the continuous updating
necessary to maintain the moving objects is such that subject cannot
encode more than one object's worth of velocity info.  Perhaps if we
reduce the demand of the task--e.g., by slowing the objects down--we
should see an increase in accuracy.  Of course, this is not a
particularly exciting prediction: if the task gets easier,
performance goes up.  What would be more interesting would be a
counterintuitive prediction.

----------------------------------------------------------------------

February 11, 2005:

Been a while...

Currently working on ShiftTrack7, which studies the influences of
top-down influences on reacquisition strategies, and FlashTrack1,
which looks at the effects of luminance shifts on tracking.

Last night, I remembered that I need a within-subject comparison of
asynch vs. synch performance, so let's put together ShiftTrack9 for
this purpose.

----------------------------------------------------------------------

June 13, 2005:

From discussions with TSH: There are three future directions for the
SpeedSearch studies.

1. Increase absolute speed with a fixed absolute difference between
   fast and slow stimuli.  See if the search ever becomes inefficient
   (it'll be surprising if it doesn't), and whether an asymmetry ever
   appears.

2. Systematically increase the noise vector from SpeedSearch 3 and see
   if the search ever becomes inefficient.

3. Start piloting SpeedTrack (and maybe run a version of SpeedSearch
   with random directions)

----------------------------------------------------------------------

September 26, 2005:

Once again, it's been a while...

We seem to have finished of the ShiftTrack and StopTrack studies for
the time being, at least until we get some reviews back on the
submitted manuscript.

In the meantime, I have been focusing on SpeedTrack and SpeedSearch
studies, to see if velocity differences that aid search also aid
tracking.  So far, we have replicated Ivry & Cohen's (1992) result in
a search task with stimuli that move randomly and bounce off walls,
and shown that a similar fast-moving benefit exists in MOT.  We
should probably replicate with different set of speeds.

What we really need in this direction is a good theory of what's going
on.  I am developing a model of MOT (see below), but it's not directly
relevant at the moment.  Generally, what we think is happening with
the speed manipulation is that the fast-moving stimuli are more
salient than slow-moving stimuli, which aids both tasks.  We need to
pin our thoughts down here, though.  What we are really interested in
is whether there is a general-purpose, "early", low-level, automatic,
parallel extraction of a basic set of perceptual features which then
is available to any visual task, and that we can detect the use of
its output in a variety of perceptual tasks.

I am also starting to develop a toy model of the MOT task that
assumes people only track one object at a time, but have markers
available to store the locations of unattended objects.  The idea is
that an attentional process is bouncing around from one target to the
other, encoding its present location and retaining that location.
When it returns to a previously marked location, it can search around
that spot, find the closest stimulus, assume it is a target, and mark
that location.  This is a complicated process, but it doesn't require
four independent visual indexes or FINSTs to support tracking.

----------------------------------------------------------------------

September 28, 2005:

Yesterday, I had the idea (inspired by Todd's concluding paragraph to
the OPAM2005 summary) to test whether differences in velocity lead to
texture segmentation, and whether there is an analagous asymmetry in
such a task.  Theoretically, this is appealing because search tasks
depend on guidance, in addition to early perceptual processes, whereas
texture segmentation is thought to rely only on early processes.
Empiricially, this potentiall gives us a second confirmation of the
asymmetry, and in a case that doesn't involve search.  As an added
benefit, we can test between Ivry & Cohen and Rosenholtz's
explanations: Ivry & Cohen predict that the speed asymmetry should
show up in texture segmentation as well; Rosenholtz predicts it
should show up only if stimuli move in multiple directions.

----------------------------------------------------------------------

September 29, 2005:

Re: texture segmentation and movement speed.  Does an asymmetry even
make sense in this case? You are segmenting things moving at one
speed from things moving at a different speed, so how could there be
an asymmetry?  In order to expect one, we must assume that
segmentation involves picking a region out of a background.  This
suggests that we should prefer Jeremy's method of identifying the
orientation of a patch instead of Todd's method of a 4AFC
identification of the quadrant the patch is located in.  The former
method requires that you pick out a target region from a distractor
region, so an asymmetry is at least conceivable.

----------------------------------------------------------------------

October 4, 2005:

One of the chief problems that has cropped up in the speedtrack
studies is an alternate explanation of the asymmetry in the tracking
data.  Our preferred explanation is the "common mechanism"
explanation, which claims that a common, early stage of perceptual
processing enables efficient search for and tracking of fast-moving
targets.  The troublesome alternative explanation is the
"efficient-search" explanation, which claims that search is efficient
for fast targets, which leads to more efficient target recovery on
fast-target MOT trials than on slow-target MOT trials.  We know that
fast targets among slow distractors are easier to find than slow
targets among fast distractors based on Ivry & Cohen and our own
findings.  So, in tracking, when subjects lose targets during the
tracking interval and attempt to recover them, it is easier to find
the fast targets than to find the slow targets simply because the
search is more efficient.  The efficient-search explanation is much
less interesting than one that claims a common stage of processing
underlies both tracking and search.  (Note that this alternative
hypothesis is different from the claim that the tracking task is
easier because the stimuli are discriminable based on speed: this
claim explains the advantage of target-fast and target-slow trials
over the same-speed trials, but doesn't explain the advantage of
target-fast trials over target-slow trials.)

So, how do we test the two alternate explanations?  During an extended
tracking interval, Os should, on average, lose more targets as more
time passes.  Thus, if there is a mechanism for recovering them, then
that mechanism should be used more frequently as the tracking interval
is lengthened.  If the recovery mechanism is more efficient in one
condition than in a second condition, then the difference in tracking
accuracy between conditions should increase as tracking continues.
According to the efficient-search explanation, then, the difference
in accuracy between fast-target trials (with efficient target
recovery) and slow-target trials (with inefficent target recovery)
should increase as trial duration increases.

Thus, in our next speedtrack experiment (speedtrack4), we will
manipulate tracking interval and compare performance on fast-target,
slow-target, and mixed trials.  (We'll drop all-slow and all-fast
trials to simplify the design, but keep mixed in order to have a
control condition.  Mixed trials seem a better control condition than
all-slow and all-fast because they still have variable speed, but no
speed-based target-distractor segregation.)  This experiment was
Todd's idea on October 3, 2005.

The main problem with this solution is that, if the common mechanism
explanation is correct and the efficient-search explanation is
incorrect, then we will find a null interaction between condition and
trial duration---these are never considered very exciting.  Another
problem is that both explanations could be correct, in which case we
would find an interaction---the saving grace here is that the
efficient-search explanation cannot account for differences between
fast- and slow-target at very short tracking intervals.  However, we
need to think of a study in which the common mechanism explanation
predicts an effect that differs from the one predicted by the
efficient-search explanation.  One possibility is to look for
dual-task costs between the speed-tracking and an separate, concurrent
search task.  Again, the problem here is that the common mechanism
explanation predicts no effect (i.e., no dual-task interference).

----------------------------------------------------------------------

October 26, 2005: Notes following my practice talk for OPAM

We discussed some experimental ideas during and following my practice
talk today.  Todd realized that I should have run speedtrack04---which
looked at the speed tracking asymmetry as tracking duration
increase---with both all-slow and all-fast conditions as controls
(instead of the mixed condition).  This is because the efficient
search explanation (see previous posting) predicts a growing advantage
of slow-targets over all-slow, because in the slow-targets condition
you can search for lost targets and recover them, whereas you cannot
do so in the all-slow condition.  We observed no difference between
these two conditions in the original speedtrack study, but they may
become more apparent at longer tracking durations.

Evan also suggested an alternative explanation.  It's possible that
the fast-moving stimuli "grab" attention in all tracking conditions.
This would make tracking easier in the fast-targets conditions, but
would cause attention to keep getting pulled away in the slow-targets
condition.  Note that this isn't the same as the common mechanism
explanation, which claims that targets are more easily segregated from
distractors when they're moving faster than when they're moving
slower, but in both cases you're able to group by speed.  Evan
suggested testing this by varying the number of distractors: if his
hypothesis (which he doesn't like) is right, then accuracy should
decline with increasing distractors in the slow-targets condition,
but not in the fast-targets condition (where the distractors are
readily filtered).  I need to make sure that this is a different
prediction from what the common mechanism makes.

We also discussed ways of improving the talk, mostly oriented towards
reducing the background and demos and spending more time discussing
all the potential pitfalls of this experimental task (e.g., Os are
not tracking, they're just picking up the fast-moving targets at the
end of the trial) and setting up strong expectations for the results
we didn't get.  Unfortunately, I have to go home, so I can't write
more on improving the talk just now...

----------------------------------------------------------------------

June 6, 2006:

Met with TSH about progress on SpeedSearch.  Presented various ideas
and decided we were on the right track to have a decent paper.  The
ideas, in decreasing order of immediacy:

1. finish SpeedSearch07, which fixes distractor speed and compares
   targets going faster or slower than it
2a. replicate SpeedSearch07 with more widely spaced speeds (e.g., 3.2
   and 9.6 for targets, 6.4 for distractors)
2b. use target speeds of 3.2 and 9.6, with a distractor speed that's
   closer to the fast target speed: Ivry & Cohen make opposite
   predictions from Ruth, in this case
3. run the lifetime search study with good speeds and lifetime ranges,
   probably with a background bright enough to hide motion trails
4. use a fixed target speed and use a distractor speed faster or
   slower than it (2a/3 reversed)
5. come up with a stimulus identification and response preparation
   complexity manipulation

Maybe there was more.  In the meantime, start writing up ProbeTrack studies.


----------------------------------------------------------------------

March 7, 2007:

Regarding ProbeTrack

A couple ideas for fixing analysis problems:

1. try fitting weibull to just target-probe trials (instead of data
   collapsed across probe-type)

   2007-03-07: tried this and didn't get much difference

2. try fitting weibull to averaged data, then comparing each subject
   to the estimated baseline

   2007-03-12: this won't work, dummy: while the average fit is quite
   clean, each individual subject is quite far from the average.

----------------------------------------------------------------------

March 16, 2007:

Ideas on DriftSearch studies:

This is basically no different from the original speedsearch studies,
although the layout of the stimuli may help counteract the global
motion effect observed in those studies.  However, the design of the
current study improves on the previous designs by comparing the
oscillating condition of Ivry & Cohen with a unidirectional motion
condition for which Ivry & Cohen's predictions differ from those of
Rosenholtz.  This could probably be implemented with the original
speedsearch stimuli.

The current version includes slow- and fast-target conditions in both
oscillating and unidirectional motion conditions.  Here are some
potential controls:

1. The oscillation has a fixed amplitude, which means that
   faster-moving stimuli oscillate at higher frequently than
   slower-moving stimuli.  Ivry & Cohen argue that this confound is
   not driving the observed asymmetry, but they also control for it,
   which I should do as well.  This may be accomplished by fixing the
   oscillation frequency, or by randomizing it over a range such that
   the range of amplitudes for slow and fast stimuli overlap.

2. Another potential confound is that the oscillating stimuli both
   oscillate and move in different directions.  Replace the
   oscillating condition with a condition in which objects move in
   both directions.

3. Fixed direction provides more time with motion signal than
   oscillatory, so limit exposure duration and see if the asymmetry is
   still absent.

# 1 is probably of peripheral concern, so focus on 2 and 3.  Of
# course, 2 solves 3

----------------------------------------------------------------------

March 27, 2007:

Further Thoughts on DriftSearch

A difference between unidirectional and multidirectional motion is
that common direction/speed may be readily grouped.  If this is the
case, then Ivry & Cohen's proposal may still be correct, but the easy
grouping afforded by unidirectional motion overrides the difficulty of
detecting a slower-moving object.  So, we need to think of a way to
break up this easy grouping.

One strategy is for the distractors move at multiple speeds, but with
a factor of two, at minimum separating the distractor speeds from the
target speed (e.g., target moves at 1 pixel/frame, distractors move at
2, 3, and 4 pixels/frame).  This is an imperfect control, since the
distractors could still be grouped, but it couldn't be as simple a
process as one that could group distractors moving at a common speed.
Perhaps try this with slow targets moving at 3 pixels/frame, fast
distractors at 6, 7, and 8 pixels/frame, and fast targets moving at 6
pixels/frame, slow distractors at 1, 2, and 3 pixels/frame.  (Todd
points out that we only need two distractor speeds.)

Another strategy is to try to find relative speeds at which search for
unidirectional slow targets becomes inefficient.  If speed detection
follows the Weber rule, then 1 vs. 2 should be equivalent to 4 vs. 8,
and 7 vs. 8 should be much harder.  In principle, it may be that, with
smaller proportional differences in speed, there may actually be a
slope for detecting targets with unidirectional motion.  This one has
the advantage that it actually tests Rosenholtz's hypothesis, and has
the potential to disprove it.

----------------------------------------------------------------------

May 18, 2007:

More ideas on ProbeTrack:

Fit a model that actually implements our theory about what is
occurring: that is, one which assumes that there is a delay of X ms
before tracking resumes.  Thus, RT reflects a baseline response time
plus the time required to reacquire the targets after the stimuli
reappear.  This leads to a formula like

RT = t + max(r - soa, 0)

where RT is the observed reaction time, t is the baseline response
time, r is the reacquisition time, and soa is the SOA.

----------------------------------------------------------------------

May 21, 2007:

Even better is a version of the above with distribution assumptions,
such that t (base RT) follows a gamma distributions with parameters
alpha and beta and r (reacquisition time) follows an exponential
distribution with parameter tau (or lambda).

----------------------------------------------------------------------

July 23, 2008:

Revisiting GetMoving (the following is a summary written for MVW to
include in his lab book):

The GetMoving studies used a multiple-object tracking (MOT) task to
investigate the extraction of motion information from a set of
stimuli.  The purpose was to follow-up on the StopTrack studies, which
investigated how people recovered targets in an MOT task when all the
stimuli disappeared for a brief gap.  In the StopTrack studies, we
found that people recover targets better following the gap when there
is motion information before the gap than when all stimuli remain
still before the gap.  The conclusion was that participants can use
motion information to recover targets, contradicting Keene and
Pylyshyn (2006).

The GetMoving studies are intended to investigate just how much motion
information people need to use it to recover targets after a gap.
Brief summaries of the studies completed so far are below:

* GetMoving01 compared performance with 2 or 4 targets with 0, 4, or 8
  frames (approx. 0, 53, or 106 ms) of motion preceding the gap (gap
  was about 293 ms).  16 participants were completed, with 4 dropped
  due to poor performance (<70% correct) in an early baseline block (1
  additional participant failed to complete the study).  The results
  are that d' improves between 0 and 4 frames, then remains unchanged
  between 4 and 8 frames.  This suggests that a mere 4 frames of
  motion is enough to extract motion trajectories and anticipate
  post-gap reappearance positions.
* GetMoving02 compared performance with motion durations of 0, 1, 4,
  8, or 200 frames.  Only 5 participants were completed, at which
  point it was realized that no reliable effects were emerging.
  Participants always tracked 4 targets, and there may be no advantage
  to pre-gap motion with 4 targets.  Future studies need smaller
  tracking loads (e.g., 2 targets).
* GetMoving03 was just GetMoving02 with an additional 2-target
  condition.  11 participants were completed, and one had to be
  dropped for poor performance (2 additional participants failed to
  complete the study).  The results are that d' improves between 1 and
  4 frames of motion, and is consistent between 0 and 1 frames, and
  between 4-200 frames (one small problem is that there is no reliable
  difference between 1 and 200 frames).  This is, in essence,
  consistent with the findings of GetMoving01.

Future studies should not mix motion durations of different orders of
magnitude (i.e., less that 10 frames and 200 or more frames is bad).
Perhaps a baseline block with around 200 frames of motion, and later
blocks with 0-4 frames of motion to see where the shift occurs.

----------------------------------------------------------------------

July 25, 2008:

Revisiting Probetrack.

I am revisiting the paper, and am surprised by how complete it is.  It
is a pretty compressed draft, and I think the big decision must be
between trying to continue compressing it (which would require some
extensive thinking/work on the figures, since there's a lot of
graphical info) or to expand it into a JEP/P&P style paper.

The paper seems mostly done.  Exp 4 needs to be revised for
Probetrack6b (instead of Probetrack4/3B and Probetrack5/3C), and the
discussion needs to be fleshed out.

Steps:
1. Complete/clean-up Probetrack6b analyses.
2. Re-write Exp 4 section.
3. Complete the discussion.
4. Add a paragraph describing the method in general terms.
5. Convert to MSWord and send to TSH.

----------------------------------------------------------------------

December 11, 2008:

Probetrack:

All of the previous steps have been completed.  The paper's discussion
needs to be expanded, and the mention of task-switching revised to
reflect the observations of ~50-ms switch costs, which are remarkably
close to the values we observed.

Also, I just had the idea of an obvious control experiment: What if we
used Scholl & Pylyshyn's (1999) occlusion-based disappearance, and
varied probe delay following disocclusion?  Because occlusion should
not trigger task-postponement mechanisms, one would expect to observe
no post-reappearance slowing at small probe delays.

----------------------------------------------------------------------

December 12, 2008:

MotionSearch:

One potential problem with the MotionSearch stimuli is that the
gratings are black and white, so there is a potential for phosphor
decay issues: the black segments of fast-moving stimuli may be
brighter than the black segments of slow-moving stimuli.  This could
be tested by changing the black segments to gray segments, and
adjusting the background color.
